{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a67f80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0b72f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageReader(object):\n",
    "    def __init__(self):\n",
    "        self.DATADIR = './PetImages'\n",
    "        self.categories = ['Dog', 'Cat']\n",
    "        self.img_width = 56\n",
    "        self.img_height = 56\n",
    "    \n",
    "    def read_images(self):\n",
    "        self.images = []\n",
    "        for category in self.categories:\n",
    "            path = os.path.join(self.DATADIR, category)\n",
    "            class_num = self.categories.index(category)\n",
    "            for img in os.listdir(path):\n",
    "                \n",
    "                try:\n",
    "                    img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                    new_array = cv2.resize(img_array, (self.img_width, self.img_height))\n",
    "                    self.images.append([new_array, class_num])\n",
    "                except:\n",
    "                    os.remove(os.path.join(path, img))\n",
    "                    pass\n",
    "    \n",
    "    def get_train_and_test(self):\n",
    "        train_data, test_data = train_test_split(self.images, test_size = 0.2, random_state = 1234)\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for element in train_data:\n",
    "            X_train.append(element[0])\n",
    "            y_train.append(element[1])\n",
    "        \n",
    "        X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "        X_train = torch.LongTensor(X_train)/255.0\n",
    "        y_train = torch.LongTensor(y_train)\n",
    "        train_tensor = TensorDataset(X_train, y_train)\n",
    "\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        for element in test_data:\n",
    "            X_test.append(element[0])\n",
    "            y_test.append(element[1]) \n",
    "        \n",
    "        X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "        X_test = torch.LongTensor(X_test)/255.0\n",
    "        y_test = torch.LongTensor(y_test)\n",
    "        test_tensor = TensorDataset(X_test, y_test)\n",
    "\n",
    "        return train_tensor, test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b685312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RN, self).__init__()\n",
    "        # We will need 2 convolutional layers, one pooling (twice) and two fully connected\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Here we define the real arquitecture\n",
    "        # Over a conv, pass a relu activation and then pooling\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        # view is used to reshape the tensor x into a 2D tensor of shape \n",
    "        x = x.view(-1, 64 * 14 * 14)\n",
    "        # FC with rely\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        # FC without any activation\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "483776cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [2500/19956], Loss: 0.6369\n",
      "Epoch [1/10], Step [5000/19956], Loss: 0.5877\n",
      "Epoch [1/10], Step [7500/19956], Loss: 0.8492\n",
      "Epoch [1/10], Step [10000/19956], Loss: 0.7626\n",
      "Epoch [1/10], Step [12500/19956], Loss: 0.5438\n",
      "Epoch [1/10], Step [15000/19956], Loss: 0.0787\n",
      "Epoch [1/10], Step [17500/19956], Loss: 0.9051\n",
      "Epoch [2/10], Step [2500/19956], Loss: 0.6428\n",
      "Epoch [2/10], Step [5000/19956], Loss: 0.0575\n",
      "Epoch [2/10], Step [7500/19956], Loss: 0.6602\n",
      "Epoch [2/10], Step [10000/19956], Loss: 0.5484\n",
      "Epoch [2/10], Step [12500/19956], Loss: 0.4062\n",
      "Epoch [2/10], Step [15000/19956], Loss: 0.0265\n",
      "Epoch [2/10], Step [17500/19956], Loss: 0.3711\n",
      "Epoch [3/10], Step [2500/19956], Loss: 1.3099\n",
      "Epoch [3/10], Step [5000/19956], Loss: 0.0046\n",
      "Epoch [3/10], Step [7500/19956], Loss: 0.4589\n",
      "Epoch [3/10], Step [10000/19956], Loss: 0.4568\n",
      "Epoch [3/10], Step [12500/19956], Loss: 0.2218\n",
      "Epoch [3/10], Step [15000/19956], Loss: 0.0386\n",
      "Epoch [3/10], Step [17500/19956], Loss: 0.1671\n",
      "Epoch [4/10], Step [2500/19956], Loss: 0.6650\n",
      "Epoch [4/10], Step [5000/19956], Loss: 0.0006\n",
      "Epoch [4/10], Step [7500/19956], Loss: 0.4425\n",
      "Epoch [4/10], Step [10000/19956], Loss: 0.4501\n",
      "Epoch [4/10], Step [12500/19956], Loss: 0.2378\n",
      "Epoch [4/10], Step [15000/19956], Loss: 0.0111\n",
      "Epoch [4/10], Step [17500/19956], Loss: 0.1401\n",
      "Epoch [5/10], Step [2500/19956], Loss: 0.4119\n",
      "Epoch [5/10], Step [5000/19956], Loss: 0.0002\n",
      "Epoch [5/10], Step [7500/19956], Loss: 0.1946\n",
      "Epoch [5/10], Step [10000/19956], Loss: 0.5101\n",
      "Epoch [5/10], Step [12500/19956], Loss: 0.2122\n",
      "Epoch [5/10], Step [15000/19956], Loss: 0.0145\n",
      "Epoch [5/10], Step [17500/19956], Loss: 0.1056\n",
      "Epoch [6/10], Step [2500/19956], Loss: 0.1528\n",
      "Epoch [6/10], Step [5000/19956], Loss: 0.0000\n",
      "Epoch [6/10], Step [7500/19956], Loss: 0.4165\n",
      "Epoch [6/10], Step [10000/19956], Loss: 0.3574\n",
      "Epoch [6/10], Step [12500/19956], Loss: 0.2105\n",
      "Epoch [6/10], Step [15000/19956], Loss: 0.0078\n",
      "Epoch [6/10], Step [17500/19956], Loss: 0.0259\n",
      "Epoch [7/10], Step [2500/19956], Loss: 0.2820\n",
      "Epoch [7/10], Step [5000/19956], Loss: 0.0000\n",
      "Epoch [7/10], Step [7500/19956], Loss: 0.1832\n",
      "Epoch [7/10], Step [10000/19956], Loss: 0.4854\n",
      "Epoch [7/10], Step [12500/19956], Loss: 0.0809\n",
      "Epoch [7/10], Step [15000/19956], Loss: 0.0566\n",
      "Epoch [7/10], Step [17500/19956], Loss: 0.0316\n",
      "Epoch [8/10], Step [2500/19956], Loss: 0.5021\n",
      "Epoch [8/10], Step [5000/19956], Loss: 0.0000\n",
      "Epoch [8/10], Step [7500/19956], Loss: 0.4413\n",
      "Epoch [8/10], Step [10000/19956], Loss: 0.4013\n",
      "Epoch [8/10], Step [12500/19956], Loss: 0.1250\n",
      "Epoch [8/10], Step [15000/19956], Loss: 0.0137\n",
      "Epoch [8/10], Step [17500/19956], Loss: 0.0252\n",
      "Epoch [9/10], Step [2500/19956], Loss: 0.4354\n",
      "Epoch [9/10], Step [5000/19956], Loss: 0.0000\n",
      "Epoch [9/10], Step [7500/19956], Loss: 0.5159\n",
      "Epoch [9/10], Step [10000/19956], Loss: 0.2449\n",
      "Epoch [9/10], Step [12500/19956], Loss: 0.1078\n",
      "Epoch [9/10], Step [15000/19956], Loss: 0.0000\n",
      "Epoch [9/10], Step [17500/19956], Loss: 0.0408\n",
      "Epoch [10/10], Step [2500/19956], Loss: 0.3983\n",
      "Epoch [10/10], Step [5000/19956], Loss: 0.0000\n",
      "Epoch [10/10], Step [7500/19956], Loss: 0.3478\n",
      "Epoch [10/10], Step [10000/19956], Loss: 0.2166\n",
      "Epoch [10/10], Step [12500/19956], Loss: 0.0137\n",
      "Epoch [10/10], Step [15000/19956], Loss: 0.0037\n",
      "Epoch [10/10], Step [17500/19956], Loss: 0.0074\n",
      "Loss per epoch:\n",
      "Epoch 1: 0.653689572499993\n",
      "Epoch 2: 0.5724416136383119\n",
      "Epoch 3: 0.5272287942435745\n",
      "Epoch 4: 0.49200971516486314\n",
      "Epoch 5: 0.4554271220918279\n",
      "Epoch 6: 0.4194181128373692\n",
      "Epoch 7: 0.3839921520426359\n",
      "Epoch 8: 0.3458336658802492\n",
      "Epoch 9: 0.3189228217833344\n",
      "Epoch 10: 0.2878481557610563\n",
      "Test Accuracy: 70.47504509921828 %\n"
     ]
    }
   ],
   "source": [
    "Lector = ImageReader()\n",
    "\n",
    "Lector.read_images()\n",
    "\n",
    "train, test = Lector.get_train_and_test()\n",
    "\n",
    "train_loader = DataLoader(train)\n",
    "test_loader = DataLoader(test)\n",
    "\n",
    "model = RN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "epocas_dict = {}\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    perdida = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Clean gradient before new batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Call backward propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 2500 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "        perdida += loss.item()\n",
    "    epocas_dict[epoch] = perdida/len(train_loader)\n",
    "\n",
    "print('Loss per epoch:')\n",
    "for key in epocas_dict:\n",
    "    print('Epoch {}: {}'.format(key+1, epocas_dict[key]))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Test Accuracy: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa9e03d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
